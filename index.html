<!DOCTYPE html>
<html>

<head>
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&display=swap">
  <title>Portfolio - HL</title>
</head>

<body>
<h4 align="center">
    <a href = "https://leeharry709.github.io/about-me/">ABOUT ME</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a href = "https://leeharry709.github.io/experience/">EXPERIENCE</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a href = "https://leeharry709.github.io/portfolio/">PORTFOLIO</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a href = "https://drive.google.com/file/d/1Elx4VLdnwMQEPpfn6MDWMcGXixWZfCvu/view">RESUME</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a href = "https://leeharry709.github.io/contact/">CONTACT</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</h4>

<div class="center-image">
  <img src="https://raw.githubusercontent.com/leeharry709/portfolio/main/banner3.jpg">
</div>


<div class="text-container">
  <h2><a href="https://github.com/leeharry709/housing_price_prediction_model">Housing Price Prediction - Exploratory Data Analysis and Advanced Regression</a></h2>

  <button id="my-btn1">Expand</button>

  <div id="container1" style="display: none;">
    <h1 id="housing-price-prediction---exploratory-data-analysis-and-predictive-model-creation">Housing Price Prediction - Exploratory Data Analysis and Advanced Regression</h1>
      <p>To view the notebook with visualizations, use this link: <a href="https://nbviewer.org/github/leeharry709/housing_price_prediction_model/blob/main/housing_price_exploration_and_prediction_model.ipynb">https://nbviewer.org/github/leeharry709/housing_price_prediction_model/blob/main/housing_price_exploration_and_prediction_model.ipynb</a></p>
      <p>The purpose of this project was to practice common, yet influential, data exploration techniques as well as creating a predictive model complete with testing 4 different models, hyperparameter tuning, principal component analysis (PCA), feature engineering, and ensembling in order to find the best model for the data. The data used was from a Kaggle competition. After submitting, I got 1243 out of 4023 teams.</p>
      <p align="center">
        <img src="https://github.com/leeharry709/housing_price_prediction_model/blob/main/submission_placement.png?raw=true" width="75%">
      </p>

      <h2 id="explanation-of-dataset">Explanation of Dataset</h2>
      <p>The dataset for the housing price prediction project consists of two main files: &quot;train.csv&quot; (the training set) and &quot;test.csv&quot; (the test set), along with a detailed data description in &quot;data_description.txt.&quot; The dataset contains various features related to properties, including factors like building class, zoning classification, lot size, and property condition. The target variable for prediction is &quot;SalePrice,&quot; representing the property&#39;s sale price in dollars. This dataset provides a comprehensive set of attributes to analyze and build predictive models for housing prices.</p>
      <h2 id="basic-data-exploration---exploratory-data-analysis-eda">Basic Data Exploration - Exploratory Data Analysis (EDA)</h2>
      <p>The data exploration was to understand the dataset, the differences between the categorical and numerical columns, and to understand how the metrics affect the target variable &quot;SalePrice.&quot; In this section, I will explore the dependent variable as well as answer some questions one might have when looking at the dataset.</p>
      <h3 id="dependent-variable-exploration">Dependent Variable Exploration</h3>
      <p>Because the end goal was to create a predictive model using this data, I came into this phase thinking about what will need to be done: Normalization and reducing the impact of outliers. Not all datasets need normalization. But, a dataset such as housing information will require it because of scale. In this dataset, there are variables such as how many bathrooms there are (typically 1-3) vs. the sale price of a home (can be over 1 million). By normalizing the data, I will improve the final performance of the model.</p>
      <h3 id="research-questions">Research Questions</h3>
      <p>For this project, I had 8 questions that I visualized in the notebook using Plotly graphical objects:
      <br>
      <br><b>Q1. What is the distribution of dwelling types and their relation to Sale Price?</b>
      <br><b>A1.</b> Single family homes (1Fam) are by far the most popular option for a home and also are the highest in price. Townhouse end units (TwnhsE) are the second most popular and cost similarly to single family homes. It is interesting to note that townhouse end units are far more expensive than standard townhouses.</p>
      <p><br><b>Q2. Does zoning impact Sale Price?</b>
      <br><b>A2.</b> Zoning can grealy impact the sale price. Floating village residential (FV) are the most expensive, followed by residential low-density (RL). There are a few zones that are not listed, most likely due to not having a substantial amount of data.</p>
      <p><br><b>Q3. Does street and alley access types impact Sale Price?</b>
      <br><b>A3.</b> Yes, paved streets and alleys are more expensive than unpaved streets and alleys.</p>
      <p><br><b>Q4. Does property shape and contour impact Sale Price?</b>
      <br><b>A4.</b> Yes, a moderately irregular property shape is the most expensive, and the regular property shape is the least expensive. Hillside houses with significant slope from side to side are the most expensive property contour type.</p>
      <p><br><b>Q5. Is there a correlation between Property Age and Sale Price?</b>
      <br><b>A5.</b> There is a negative correlation between Property Age and Sale Price. As Property Age goes up, Sale Price typically goes down.</p>
      <p><br><b>Q6. Is there a correlation between Living Area and Sale Price?</b>
      <br><b>A6.</b> There is a strongly positive correlation between Living Area and Sale Price. As Living Area goes up, Sale Price typically goes up.</p>
      <p><br><b>Q7. Does price change year to year?</b>
      <br><b>A7.</b> For the years 2006 to 2010, there was not a significant change in yearly average Sale Price of homes.</p>
      <p><br><b>Q8. What is the correlation between Sale Price and all numerical features?</b>
      <br><b>A8.</b> Sale Price is most negatively correlated with kitchen above grade (ground) (KitchenAbvGr) and enclosed porch (EnclosedPorch). Sale Price is most positively correlated with living area above grade square footage (GrLivArea) and overall quality (OverallQual)</p>
      <h2 id="model-creation---predictive-modeling">Model Creation - Predictive Modeling</h2>
      <p>For experimentation, I used 4 different regression techniques: Linear Regression, Random Forest, Gradient Boosting via XGBoost, and Multi-Layer Perceptron (MLP). Some of the models work better for this project, while others do not. It is important to understand the strengths and weaknesses of each model while approaching the problem.
      <br>
      <br><b>Linear Regression</b> is a simple regression technique that is quick and easily interpretable and can test assumptions such as linearity and homoscedasticity, which describes the same variance between error terms across all values of the independent variables.
      <br><b>Random Forest</b> is an ensemble method that can capture complex non-linear relationships between features and the target variable. It is able to handle categorical variables well.
      <br><b>XGBoost (Extreme Gradient Boosting)</b> is known for its high predictive performance and efficiency, making it suitable for large datasets. It also ahs built-in support for handling missing data, assisting in final model performance.
      <br><b>MLP (Multi-Layer Perceptron)</b> is a complex neural network-based regression technique that can automatically learn and extract relevant features from raw data, accommodating complex datasets.</p>
      <h3 id="data-preprocessing---creating-a-pipeline">Data Preprocessing - Creating a Pipeline</h3>
      <p>For preprocessing the dataset, creating a pipeline greatly increased consistency by automating preprocessing steps such as missing value imputation, one-hot encoding for categorical variables, PCA component selection, and feature scaling. Creating a pipeline will also set up the project for hyperparameter tuning, and model comparison.</p>
      <p>In this step, I implemented a data preprocessing pipeline to prepare the dataset for machine learning modeling. I defined separate transformers for numerical and categorical columns. For numerical data, I imputed missing values with the mean and scaled the data to ensure uniformity. For categorical data, I imputed missing values with a new category (&#39;missing&#39;) and performed one-hot encoding to convert categorical variables into a binary format. I identified and separated the categorical and numerical columns in the dataset, and then combined these transformers using the ColumnTransformer. Finally, I created a comprehensive pipeline that applies these preprocessing steps to the dataset, preparing it for subsequent model training. Additionally, I normalized the dependent variable, &#39;SalePrice,&#39; by taking its logarithm to address skewness observed during data exploration.</p>
      <h3 id="fitting-and-hyperparameter-tuning-models">Fitting and Hyperparameter Tuning Models</h3>
      <p>By fitting the models to the training data, I aimed to enable them to learn from the patterns in the data. Hyperparameter tuning allowed me to systematically search for the best combination of model settings, ensuring that each model performed at its highest potential, resulting in improved predictive accuracy and more reliable housing price predictions.</p>
      <p>I split the data into training and testing sets to facilitate model evaluation. Then, I defined four regression models: Linear Regression, Random Forest, XGBoost, and MLP (Multi-Layer Perceptron), each with their respective hyperparameter grids. To assess model performance, I employed 3-fold cross-validation and utilized GridSearchCV to search for the best hyperparameters for each model. The results, including the best parameters and root mean squared error (RMSE) scores, were printed to the console. This process allows for the systematic comparison and selection of the most suitable regression model for the housing price prediction task.</p>
      <h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h3>
      <p>Principal Component Analysis (PCA) reduces the dimensionality of the dataset. PCA helped me simplify the data by identifying the most important patterns and features while preserving as much variance as possible. This not only improved computational efficiency but also reduced the risk of overfitting, ultimately enhancing the accuracy and stability of the predictive models.</p>
      <p>I started by fitting PCA to the preprocessed data and then calculated the cumulative explained variance to determine the optimal number of components to retain (in this case, enough to explain at least 95% of the variance). I integrated PCA into my data preprocessing pipeline and transformed the dataset accordingly. With the reduced-dimension dataset, I re-ran my regression models while performing hyperparameter tuning to optimize their performance. This approach aimed to improve computational efficiency and potentially enhance model accuracy by focusing on the most informative components of the data.</p>
      <h3 id="feature-engineering">Feature Engineering</h3>
      <p>Feature engineering is essential in machine learning because it allows you to create new, informative features from existing data, ultimately enhancing the performance of your models. By engineering features, you can capture relevant patterns and relationships that may not be apparent in the original data, leading to better predictive accuracy. Additionally, feature engineering can help mitigate issues like overfitting and improve the interpretability of models, making them more effective in solving complex real-world problems like housing price prediction.</p>
      <p>After adding features to the dataset such as total square footage, total bathrooms, and month sold, I re-ran my regression models while performaning hyperparameter tuning using the PCA pipeline. Combining the PCA pipeline with feature engineering allows for the creation of more informative features while reducing dimensionality, resulting in improved model interpretability and predictive accuracy.</p>
      <h3 id="ensembling---stacking-regression">Ensembling - Stacking Regression</h3>
      <p>By combining multiple diverse models such as Linear Regression, Random Forest, XGBoost, and MLP, I leverage their individual strengths and mitigate weaknesses. Stacking allows us to learn from the predictions of these base models, effectively capturing complex patterns and improving overall accuracy while reducing the risk of overfitting, thereby yielding more reliable and robust housing price predictions.</p>
      <p>I implemented a StackingRegressor ensemble to combine the strengths of Linear Regression, Random Forest, XGBoost, and MLP.The goal was to create a powerful predictive model by leveraging the diverse strengths of these models, ultimately achieving improved housing price prediction accuracy. Finally, I evaluated the best stacking ensemble on the test data, providing an RMSE score as an indication of its predictive performance.</p>
      <h2 id="results">Results</h2>
      <h4 id="normal-pipeline-rmse-without-pca-or-feature-engineering">Normal Pipeline RMSE (without PCA or Feature Engineering):</h4>
      <ol>
      <li><strong>Linear Regression</strong>: 482,591,986.50<ul>
      <li>The extremely high RMSE suggests poor performance for predicting housing prices with Linear Regression.</li>
      </ul>
      </li>
      <li><strong>RandomForest</strong>: 0.1468<ul>
      <li>Reasonable performance with relatively low prediction error.</li>
      </ul>
      </li>
      <li><strong>XGBoost</strong>: 0.1445<ul>
      <li>Good performance with predictions close to actual sale prices.</li>
      </ul>
      </li>
      <li><strong>MLP (Neural Network)</strong>: 0.1480<ul>
      <li>Slightly higher RMSE but still reasonable for housing price prediction.</li>
      </ul>
      </li>
      </ol>
      <h4 id="pca-pipeline-rmse-with-pca-but-without-feature-engineering">PCA Pipeline RMSE (with PCA but without Feature Engineering):</h4>
      <ol>
      <li><strong>Linear Regression</strong>: 0.1418<ul>
      <li>Improved performance after PCA but still relatively high RMSE.</li>
      </ul>
      </li>
      <li><strong>RandomForest</strong>: 0.1525<ul>
      <li>Slight drop in performance after PCA.</li>
      </ul>
      </li>
      <li><strong>XGBoost</strong>: 0.1453<ul>
      <li>Consistent performance after PCA.</li>
      </ul>
      </li>
      <li><strong>MLP (Neural Network)</strong>: 0.1626<ul>
      <li>Significant performance drop after PCA.</li>
      </ul>
      </li>
      </ol>
      <h4 id="feature-engineering-and-pca-pipeline-rmse">Feature Engineering and PCA Pipeline RMSE:</h4>
      <ol>
      <li><strong>Linear Regression</strong>: 0.1425<ul>
      <li>Similar performance with feature engineering and PCA.</li>
      </ul>
      </li>
      <li><strong>RandomForest</strong>: 0.1529<ul>
      <li>Minimal impact of feature engineering and PCA.</li>
      </ul>
      </li>
      <li><strong>XGBoost</strong>: 0.1396<ul>
      <li>Improved performance after feature engineering and PCA.</li>
      </ul>
      </li>
      <li><strong>MLP (Neural Network)</strong>: 0.1515<ul>
      <li>Improved performance with feature engineering and PCA.</li>
      </ul>
      </li>
      </ol>
      <h4 id="stacking-ensemble-with-feature-engineering-and-pca-pipeline">Stacking Ensemble with Feature Engineering and PCA Pipeline:</h4>
      <ul>
      <li>Best RMSE: 0.1328<ul>
      <li>The stacking ensemble outperforms individual models and other pipelines, providing the most accurate predictions for housing prices (SalePrice).</li>
      </ul>
      </li>
      </ul>
      <p>In summary, the stacking ensemble with feature engineering and PCA appears to be the most suitable choice for predicting housing prices (SalePrice). It outperforms individual models and other pipelines by providing the lowest RMSE, suggesting that it can make more accurate predictions for this specific task.</p>
   <br>
  </div>

  <br><br><h2><a href="https://github.com/leeharry709/BERT-for-Sentence-Classification">Binary Sentiment Classification - BERT for Sentence Classification</a></h2>

  <button id="my-btn2">Expand</button>

  <div id="container2" style="display: none;">
    <h1 id="binary-sentiment-classification---bert-for-sentence-classification">Binary Sentiment Classification - BERT for Sentence Classification</h1>
      <h2 id="introduction">Introduction</h2>
      <p>This project focuses on the fine-tuning of a BERT pre-trained model for the task of sentiment analysis on customer reviews. BERT (Bidirectional Encoder Representations from Transformers) models are originally trained on a vast corpus of text data to acquire comprehensive language understanding. In this endeavor, I aim to leverage the power of BERT by fine-tuning it specifically for sentiment analysis on reviews with the objective of achieving high accuracy and robust performance.</p>
      <p>The primary objective is to assess how effectively the BERT model can be adapted and fine-tuned to accurately classify reviews as either positive or negative. Through this data science endeavor, I will explore the model&#39;s ability to capture nuanced sentiment patterns within the context of customer reviews. The project involves tokenization, data preprocessing, model training, and rigorous evaluation to gauge its performance on this specific NLP task.</p>
      <h2 id="bert-model">BERT model</h2>
      <p>BERT, or Bidirectional Encoder Representations from Transformers, is a deep learning model for natural language processing (NLP). It was developed by Google AI in 2018 and has since become one of the most popular NLP models, achieving state-of-the-art results on a variety of tasks, including question answering, natural language inference, and sentiment analysis. BERT is based on the transformer architecture, which is a neural network architecture that is particularly well-suited for NLP tasks. Transformer models are able to learn long-range dependencies between words in a sentence, which is essential for understanding the meaning of text. BERT is pre-trained on a massive dataset of text and code, which allows it to learn general language representations. This pre-training makes BERT very efficient at learning new tasks, as it does not need to be trained from scratch on each new dataset.</p>
      <p align="center">
        <img src="https://stanford-cs324.github.io/winter2022/lectures/images/bert.png" width="50%">
      </p>

      <h2 id="tokenization">Tokenization</h2>
      <h3 id="introduction-1">Introduction</h3>
      <p>Tokenization is a crucial preprocessing step in natural language processing (NLP) tasks. It involves converting raw text into smaller units called tokens, which can be words, subwords, or characters. In this section, I will discuss the tokenization process used for a binary sentiment classifier based on Amazon reviews.</p>
      <h3 id="dataset-description">Dataset Description</h3>
      <p>The Amazon reviews polarity dataset used in this project was constructed by Xiang Zhang and is commonly used as a text classification benchmark. It contains reviews from Amazon and is split into positive and negative sentiment classes. For this report, a subset of the dataset provided by Kaggle user &#39;kritanjalijain&#39; was used. Due to computational constraints, only a random 10% of the data was selected for training and validation.</p>
      <h3 id="tokenization-process">Tokenization Process</h3>
      <p>To prepare the data for training, I followed these tokenization steps:</p>
      <ol>
      <li>Load the dataset from a CSV file, containing columns &#39;label&#39;, &#39;title&#39;, and &#39;review&#39;.</li>
      <li>Filter the dataset to include only reviews with a maximum length of 200 characters.</li>
      <li>Randomly select 10% of the filtered dataset for training and validation.</li>
      </ol>
      <p>To visualize the tokenization process, I took a sample review and demonstrated the following:</p>
      <ul>
      <li>The original sentence.</li>
      <li>The sentence split into tokens.</li>
      <li>The sentence mapped to token IDs.</li>
      </ul>
      <p>Additionally, I determined the maximum review length in the dataset, which was found to be <strong>200 tokens</strong>.</p>
      <h2 id="pre-training-setup">Pre-Training Setup</h2>
      <h3 id="data-preprocessing">Data Preprocessing</h3>
      <p>After tokenization, the dataset was further processed for model input. The following steps were performed:</p>
      <ol>
      <li>Truncate reviews longer than 200 tokens.</li>
      <li>Encode the text using the BERT tokenizer, adding &#39;[CLS]&#39; and &#39;[SEP]&#39; tokens, and ensuring all sentences were padded to a length of 200 tokens.</li>
      <li>Create PyTorch tensors for input IDs and attention masks.</li>
      <li>Adjust the labels to convert the dataset to binary sentiment classification (0 for negative, 1 for positive).</li>
      </ol>
      <h3 id="data-splitting">Data Splitting</h3>
      <p>The dataset was divided into training and validation sets using an 90%-10% split. This resulted in <strong>38,394 training samples</strong> and <strong>4,267 validation samples</strong>.</p>
      <h3 id="batch-size">Batch Size</h3>
      <p>A batch size of 20 was chosen for training.</p>
      <h2 id="training">Training</h2>
      <h3 id="model-training">Model Training</h3>
      <p>Training the binary sentiment classifier involved the following steps:</p>
      <ol>
      <li>Define the optimizer (AdamW) and loss function (Binary Cross-Entropy Loss).</li>
      <li>Create a learning rate scheduler.</li>
      <li>Train the model for <strong>3 epochs</strong>.</li>
      </ol>
      <h3 id="training-metrics">Training Metrics</h3>
      <p>During training, several metrics were tracked:</p>
      <ul>
      <li>Training loss: The average loss calculated over all training batches.</li>
      <li>Training time: The time taken for each epoch of training.</li>
      <li>Validation loss: The average loss calculated over all validation batches.</li>
      <li>Validation accuracy: The accuracy of the model on the validation set.</li>
      </ul>
      <p><b>Training Summary</b></p>
      <table>
      <thead>
      <tr>
      <th>epoch</th>
      <th>Training Loss</th>
      <th>Evaluation Loss</th>
      <th>Evaluation Accuracy</th>
      <th>Training Time</th>
      <th>Validation Time</th>
      </tr>
      </thead>
      <tbody><tr>
      <td>1</td>
      <td>0.21</td>
      <td>0.18</td>
      <td>0.94</td>
      <td>0:12:55</td>
      <td>0:00:27</td>
      </tr>
      <tr>
      <td>2</td>
      <td>0.10</td>
      <td>0.20</td>
      <td>0.94</td>
      <td>0:13:30</td>
      <td>0:00:30</td>
      </tr>
      <tr>
      <td>3</td>
      <td>0.04</td>
      <td>0.27</td>
      <td>0.94</td>
      <td>0:12:58</td>
      <td>0:00:28</td>
      </tr>
      </tbody></table>
      <h2 id="model-testing">Model Testing</h2>
      <h3 id="prediction-on-test-set">Prediction on Test Set</h3>
      <p>The model was evaluated on the test set using the following steps:</p>
      <ol>
      <li>Setting the model to evaluation mode.</li>
      <li>Running predictions on the test dataset, batch by batch.</li>
      <li>Storing the model&#39;s predictions and true labels.</li>
      </ol>
      <p>The prediction process resulted in <strong>predicted logits</strong> for each review.</p>
      <h3 id="calculating-accuracy-precision-and-recall">Calculating Accuracy, Precision, and Recall</h3>
      <p>To assess the model&#39;s performance, accuracy, precision, and recall were calculated using the following definitions:</p>
      <ul>
      <li>Accuracy: The proportion of correctly predicted labels among all test samples.</li>
      <li>Precision: The proportion of true positives among all positive predictions.</li>
      <li>Recall: The proportion of true positives correctly predicted among all actual positives.</li>
      </ul>
      <p>For this evaluation, these metrics were calculated individually for each batch of test data and then averaged.</p>
      <h3 id="results">Results</h3>
      <p>The evaluation metrics for the model on the test dataset are as follows:</p>
      <p><b>Testing Model</b></p>
      <table>
      <thead>
      <tr>
      <th>Metric</th>
      <th>Score</th>
      </tr>
      </thead>
      <tbody><tr>
      <td>Accuracy</td>
      <td>0.891</td>
      </tr>
      <tr>
      <td>Precision</td>
      <td>0.9027</td>
      </tr>
      <tr>
      <td>Recall</td>
      <td>0.8784</td>
      </tr>
      </tbody></table>
      <p>These metrics indicate that the binary sentiment classifier performs well on the test dataset, achieving high accuracy, precision, and recall, which are key indicators of its effectiveness in classifying Amazon reviews into positive and negative sentiments.</p>
      <h2 id="conclusion">Conclusion</h2>
      <p>The combination of tokenization, training, and testing has proven the effectiveness of the binary sentiment classifier in classifying Amazon reviews into positive and negative sentiments. It not only performed well during training and validation but also demonstrated its ability to generalize to new data during testing.</p>
      <p>This model can be a valuable tool for sentiment analysis tasks on Amazon reviews, providing accurate and reliable results. Further fine-tuning or optimization may be explored based on specific application requirements, but the foundation for sentiment classification is solid and promising.</p>
      <h2 id="potential-uses">Potential Uses</h2>
      <p>Binary classification of text is extremely powerful and relevant in today&#39;s business. One idea I had for binary sentiment classification is to analyze manager reviews of employees in a large company and identifying potential racial or gender bias. Being proactive in identifying potential bias can improve workplace satisfaction, inclusion, retention, and even avoid lawsuits.</p>
      <br>
  </div>

  <br><br><h2><a href="https://github.com/leeharry709/NLP-Text-Summary-Sentiment-Analysis/">Generative Text Summarization and Sentiment Analysis</a></h2>

  <button id="my-btn3">Expand</button>

  <div id="container3" style="display: none;">
    <h1 id="nlp-text-summary-sentiment-analysis">Generative Text Summarization and Sentiment Analysis</h1>
      <h2 id="introduction">Introduction</h2>
      <p>Text summarization is a fundamental task in natural language processing (NLP), aimed at condensing the content of a given text while preserving its most important information. In this report, I discuss a text summarization application built using the PEGASUS model, a state-of-the-art transformer-based architecture for abstractive summarization. The application also leverages additional NLP tools such as keyword extraction and sentiment analysis to provide a comprehensive summary.</p>
      <h2 id="application-overview">Application Overview</h2>
      <p>The text summarization application is designed to interactively summarize input text provided by the user. It utilizes the transformers library to load a pre-trained PEGASUS model and tokenizer. The application is built using Streamlit, a Python library for creating web applications with minimal code.</p>
      <h2 id="pegasus-model">PEGASUS Model</h2>
      <p align='center'>
        <img src='https://production-media.paperswithcode.com/methods/f8b904da-2aaa-4b73-85cd-a8adfacce042.png' width='75%'>
      </p>
      PEGASUS is a pre-trained transformer model developed by Google AI for text summarization, built on top of the BART (Bidirectional Autoregressive Transformers) architecture. PEGASUS uses a masked language modeling (MLM) objective to pre-train, which involves randomly masking some of the words in a sentence and then predicting the missing words. What is unique with the output of PEGASUS is that the model generates a completely new summary whereas other models use sentences within the input text to generate a summary.

      <h2 id="application-components">Application Components</h2>
      <p>The application consists of the following components:</p>
      <ol>
      <li><p><strong>Model Loading</strong>: The PEGASUS model and tokenizer are loaded using the Hugging Face transformers library.</p>
      </li>
      <li><p><strong>Summarization Function</strong>: The core of the application is the summarize_text function, which takes the user&#39;s input text as input and performs the following tasks:</p>
      <ul>
      <li>Tokenizes the input text.</li>
      <li>Generates a summary using the PEGASUS model.</li>
      <li>Extracts keywords from the input text.</li>
      <li>Calculates sentiment polarity and subjectivity.</li>
      </ul>
      </li>
      <li><p><strong>User Interaction</strong>: Users can input the text they want to summarize through the chat interface provided by Streamlit. The application allows for interactive communication with the summarization model.</p>
      </li>
      <li><p><strong>Output Display</strong>: The application displays the following information as output:</p>
      <ul>
      <li>The generated summary.</li>
      <li>The top 10 keywords extracted from the input text.</li>
      <li>Sentiment analysis results, including polarity and subjectivity scores.</li>
      </ul>
      </li>
      </ol>
      <p align='center'>
        <img src='https://github.com/leeharry709/NLP-Text-Summary-Sentiment-Analysis/blob/main/Screenshot%202023-08-18%20215116.png?raw=true' width='65%'>
      </p>

      <p>The image shows the summary, keywords, and sentiment of the introductory paragraph of the Google AI blog post: <a href="https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html">PEGASUS: A State-of-the-Art Model for Abstractive Text Summarization</a>.</p>
      <h2 id="summarization-and-analysis">Summarization and Analysis</h2>
      <p>Once the user provides input, the application proceeds to perform summarization and analysis tasks:</p>
      <ul>
      <li>It tokenizes the input text, generates a summary using PEGASUS, and displays the summary.</li>
      <li>It extracts the top 10 keywords from the input text and displays them.</li>
      <li>It calculates sentiment analysis scores, including polarity and subjectivity, and displays them.</li>
      </ul>
      <h2 id="conclusion">Conclusion</h2>
      <p>The text summarization application built with PEGASUS provides an efficient and user-friendly way to summarize text while also offering additional insights through keyword extraction and sentiment analysis. This application can be a valuable tool for users who want to quickly understand the essence of a given text and gain insights into its sentiment and key topics.</p>
      <p>Further improvements and customization options can be explored, such as fine-tuning the summarization model for domain-specific tasks or enhancing the user interface for a more engaging user experience.</p>
      <h2 id="real-world-application">Real-World Application</h2>
      <p>Text summarization, sentiment, and keyword extraction are all very essential to everyday life already. All three methods of NLP are used in search engines to show users before they click on the link what they are expecting in the website. The keywords can help users find what they are looking for without typing out the exact phrases or title of the webpage, the summarization can tell them briefly what the page is about before fully clicking on them (and inadvertantly avoiding spam or malware sites), and the sentiment can filter out search results that are too polarizing and opinionated. Optimization of NLP will allow users to live more efficient and productive lives.</p>
      <br>
  </div>

  <br><br><h2><a href="https://github.com/leeharry709/generative-nlp-chatbot">LangChain Agents - Generative Text AI</a></h2>

  <button id="my-btn4">Expand</button>

  <div id="container4" style="display: none;">
    <h1 id="LangChain Agents - Generative Text AI">LangChain Agents - Generative Text AI</h1>
    <h2 id="introduction">Introduction</h2>
    <p>The LangChain Agents application is a conversational AI powered by LangChain, an AI framework that combines the capabilities of various natural language processing (NLP) tools and models. This report explores the key components of the application and its functionality.</p>
    <h2 id="application-overview">Application Overview</h2>
    <p>The LangChain Agents application serves as an interactive interface for users to engage in natural language conversations with a generative text AI. The application utilizes the LangChain framework, including the LangLink Model (LLM) from OpenAI, which is fine-tuned to provide high-quality text generation.</p>
    <h3 id="application-components">Application Components</h3>
    <p>The application consists of the following components:</p>
    <ol>
    <li><p><strong>Initialization</strong>: The application is initialized with predefined settings and tools, including the LangLink Model (LLM) from OpenAI. It also loads tools for various tasks, such as language translation, sentiment analysis, and more.</p>
    </li>
    <li><p><strong>User Interaction</strong>: Users can interact with the AI through a chat interface provided by Streamlit. They can input questions, requests, or prompts in natural language.</p>
    </li>
    <li><p><strong>Conversational AI</strong>: The core of the application is the LangChain Agent, which leverages LLM and other tools to provide responses. The AI can understand user queries, generate text-based responses, and perform various tasks like web searches using the loaded tools.</p>
    </li>
    <li><p><strong>Output Display</strong>: The application displays chat messages in a conversation format, distinguishing between user and assistant messages. Responses generated by the AI are presented in real-time.</p>
    </li>
    </ol>
    <h2 id="demonstration">Demonstration</h2>
    <p align="center">
      <img src="https://github.com/leeharry709/generative-nlp-chatbot/blob/main/lanchain_agents_ss.png?raw=true" width='75%'>
    </p>

    <h3 id="user-interaction">User Interaction</h3>
    <p>The application interface begins with a message from the assistant: &quot;How can I help you?&quot; Users can initiate conversations by typing queries or prompts into the chat input field.</p>
    <h3 id="ai-responses">AI Responses</h3>
    <p>Upon receiving user input, the application processes the query using LangChain and generates a text-based response. The assistant&#39;s responses are displayed in the chat interface, creating a conversational flow.</p>
    <h2 id="conclusion">Conclusion</h2>
    <p>The LangChain Agents application demonstrates the capabilities of generative text AI and its potential for various NLP tasks. Users can engage in natural language conversations, seek information, or request assistance, making it a versatile tool.</p>
    <p>Further enhancements and customization options can be explored, such as fine-tuning the AI for specific domains, integrating additional tools, or improving the user interface for a more interactive experience. The LangChain framework, with its combination of powerful tools and models, offers the foundation for creating advanced conversational AI applications.</p>
    
  </div>
</div>

<script>
  const btn1 = document.querySelector("#my-btn1");
  const container1 = document.querySelector("#container1");

  const btn2 = document.querySelector("#my-btn2");
  const container2 = document.querySelector("#container2");

  const btn3 = document.querySelector("#my-btn3");
  const container3 = document.querySelector("#container3");

  const btn4 = document.querySelector("#my-btn4");
  const container4 = document.querySelector("#container4");

  container1.style.display = "none";
  container2.style.display = "none";
  container3.style.display = "none";
  container4.style.display = "none";

  btn1.addEventListener("click", function () {
    if (container1.style.display === "none") {
      container1.style.display = "block";
      btn1.innerHTML = 'Collapse';
    } else {
      container1.style.display = "none";
      btn1.innerHTML = 'Expand';
    }
  });

  btn2.addEventListener("click", function () {
    if (container2.style.display === "none") {
      container2.style.display = "block";
      btn2.innerHTML = 'Collapse';
    } else {
      container2.style.display = "none";
      btn2.innerHTML = 'Expand';
    }
  });

  btn3.addEventListener("click", function () {
    if (container3.style.display === "none") {
      container3.style.display = "block";
      btn3.innerHTML = 'Collapse';
    } else {
      container3.style.display = "none";
      btn3.innerHTML = 'Expand';
    }
  });

  btn4.addEventListener("click", function () {
    if (container4.style.display === "none") {
      container4.style.display = "block";
      btn4.innerHTML = 'Collapse';
    } else {
      container4.style.display = "none";
      btn4.innerHTML = 'Expand';
    }
  });
</script>
</body>
</html>
